## What is the "cache" used fpr in our implementation of forward propagation and backward propagation?
D. We use it to pass variables computed during forward propagation to the corrsponding backward propagation step. It contains useful values for backward propagation to compute derivatives.
## Among the following, which ones are "hyperparameters"?(Check all yjay apply)
number of iteration; size of the hidden layers; learning rate \alpha; number pf layers L in the neural network; <br>
参数是指模型能自己计算出来的，比如w，b，a等；而超参数是指需要人经验判断的参数，比如学习率，迭代次数，网络模型的形状，包括层数及每层有多少节点。
## Which of the following statements is true?
The deeper layers of a neural network are typically computing more complex features of the input than the earlier layers.
## Vectorization allows you to compute forward propagation in an L-layer neural network without an explicit for-loop(or any other explicit iterative loop) over the layers l=1,2,...,L. True/False?
False 对于正向传播，要遍历每一层是需要有一个显示循环的。
## Assume we store the values for n^{[l]} in an array layers, as follows: layers_dims=[n_x,4,3,2,1]. So layer 1 has four hidden units, layer 2 has 3 hidden units and so on. Which of the following for-loops will allow you to initialize the parameters for the model?
D.
## Consider the following neural network, how many layers does this network have?
A.The number of layers L is 4, and the number of hidden layers is 3.
## During forward propagation, in the forward function for a layer l you need to know what is the activation function in a layer(Sigmoid, tanh, ReLU,etc.)During backpropagation, the corrspondng backward function also needs to know what is the activation function for layer l, since the gradient depends on it. True/False?
True
## There are certain functions with the following properties:1.To compute the function using a shallow network circuit, you will need a large network(where we measure size by the number of logic gates in the network),but 2 To compute it using a deep network circuit, you need only an exponentially smaller network. True/False?
True 层数少，节点数就会指数级增长
## Consider the following 2 hidden layer neural network:
W1.shape=(4,4),b1.shape=(4,1);W2.shape=(3,4),b2.shape=(3,1);W3.shape=(1,3),b3.shape=(1,1)
## Whereas the previous question used a specific network, in the general case what is the dimension of W^{[l]}, the weight matrix associated with layer l?
W^{[l]}has the shape(n^{[l]},n^{[l-1]})
