# What does a neuron compute?
A. A neuron computes a function g that scales the input x linearly (Wx+b)<br>
B. A neuron computes the mean of all features before applying the output to an activation function<br>
C. A neuron computes a linear dunction (z=Wx+b) followed by an activation function<br>
D. A neuron computes an activation function followed by a linear function(z=Wx+b)<br>
D
# Which of these is the "Logistic Loss"?
C
# Suppose img is a (32,32,3)array, representing a 32*32 image with 3 color channels red, green and blue. How do you reshape this into a column vector?
C: x=img.reshape((32*32*3,1))
# Consider the two following random arrays "a" and "b", what will be the shape of "c"?
C: c.shape=(2,3)
# Consider the two following random arrays"a" and "b", what will be the shape of "c"?
The computation cannot happen because the sizes don't match. It's going to be "Error"!
# Suppose you have n_x input features per example. Recall that X, what is the dimension of X?
(n_x,m)
# Recall that "np.dot(a,b)"performs a matrix multiplication on a and b, whereas "a*b" performs an element-wise multiplication. Consider the two following random arrays "a" and "b", what is the shape of "c"?
D. c.shape=(12288,45)
# Consider the following code snippet: How do you vectorize this?
A. c=a+b.T
# Consider the following code: What will be c?
A. This will invoke broadcating, so b is copied three times to become(3,3), and * is an element-wise product so c.shape will be (3,3)
# Consider the following computation graph, what is the output J?
B. J=(a-1)*(b+c)
# Broadcasting的四条规则：
- 让所有输入数组都向其中shape最长的数组看齐，shape中不足的部分都通过在前面加1补齐；
- 输出的数组的shape是输入数组shape的各个轴的最大值；
- 如果输入数组的某个轴和输出数组的对应轴的长度相同或者其长度为1时，这个数组能够用来计算，否则出错；
- 当输入数组的某个轴的长度为1时，沿着此轴运算时都用此轴上的第一组值。
# Gradient Descent
- 原理：按梯度最大的方向逼近最小值；沿着全局最小值移动
几乎任意初始化都有效，无论从哪点开始初始化，都应该达到同一点或大致相同的点
