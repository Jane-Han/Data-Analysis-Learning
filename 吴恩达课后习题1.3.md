# Activation Function
tanh函数在大部分情景下较sigmoid函数好，因为无需数据中心化就可将数据的均值控制在0左右。
One exception is output layer where y=0/1, that is when we do binary classification.<br>
不同层可应用不同的激活函数，但tanh和sigmoid函数都有一个缺点，when z becomes very big or very small, the slope truns to 0,拖慢梯度下降算法。
## ReLU：recitified linear unit修正线性单元 为默认激活函数
虽然在z=0处不可导，但在编程上取到z=0的概率非常小，可以忽略不计。<br>
## If you're not sure what to use for your hidden layer, I would just use the ReLU activation function, that's what you see most people using these days.
## Why nonlinear activation function？
如果我们应用线性激活函数，只是把输入做线性组合以后输出，那么，无论你的神经网络有多少层，你simply在做对于线性激活函数的计算，不如直接去掉所有的隐藏层。<br>
Only one place where you might use linear activation function, thai is when you do machine learning on a regression problem where the output is just a real number.
