## Which of the following are true?
A.C.D.F.
## The tanh activation usually works better than sigmoid activation function for hiddent units because the mean pf its output is close to zero, and so it centers the data better for the next layer. True/False?
True.
## Which of these is a correct vectorized implementation of forward propagation of layer l, where 1<=l<=L?
B
## You're building a binary classifier for recognizing cucumbers(y=1)va.watermelons(y=0). Which one of these activation functions would you recommend using for the output layer?
C.sigmoid
## Consider the following code: What will be B.shape?
B.(4,1)
## Suppose you have built a neural network. You decide to initialize the weights and biases to be zero. Which of the following statements is true?
A. Each neuron in the first hidden layer will perform the same computation. So even after multiple iterations of gradient descent each neuron in thelayer will be computing the same things as other neurons.
## Logistic regression's weights w should be intialized randomly rather than to all zeros, because if you intialize to all zeros, then logistic regression fail to learn a useful decision boudary because ir will fail to "break symmetry".True/False?
False
## You have built a network using the tanh activation for all the hidden units. You inntialize the weights to relative large values, using np.random.rand（）*1000. What will happen?
A. This will cause the inputs of the tanh to also be very large, thus causing gradients to be close to zero. The optimization algorithm will thus become slow.
## Consider the following 1 hidden layer neural network:
BCEH
## In the same network as the previous question, what are the dimensions of Z^[1] and A[1]?
C
## Activation Function
tanh函数在大部分情景下较sigmoid函数好，因为无需数据中心化就可将数据的均值控制在0左右。
One exception is output layer where y=0/1, that is when we do binary classification.<br>
不同层可应用不同的激活函数，但tanh和sigmoid函数都有一个缺点，when z becomes very big or very small, the slope truns to 0,拖慢梯度下降算法。
## ReLU：recitified linear unit修正线性单元 为默认激活函数
虽然在z=0处不可导，但在编程上取到z=0的概率非常小，可以忽略不计。<br>
## If you're not sure what to use for your hidden layer, I would just use the ReLU activation function, that's what you see most people using these days.
## Why nonlinear activation function？
如果我们应用线性激活函数，只是把输入做线性组合以后输出，那么，无论你的神经网络有多少层，你simply在做对于线性激活函数的计算，不如直接去掉所有的隐藏层。<br>
## The composition of two linear functions is itself a linear function.
Only one place where you might use linear activation function, thai is when you do machine learning on a regression problem where the output is just a real number.
## Initialize your parameters randomly
如果将神经网络的各参数数组全部初始化为0，再使用梯度下降法，那会完全无效。<br>
因为如果你讲W初始化为0，那么因为两个隐藏单元一开始就在做同样的计算，且两个隐藏单元对输出单元的影响也一样大，那么在每一次迭代之后，同样的对称性依然存在，两个隐藏单元依然对称。<br>
No matter how long you train your neural network, both hidden units are still computing exactly the same function, so there is no point in having more than one hidden unit because they're doing the same thing.<br>
## We usually prefer to intialize the way to very small random values.我们通常将权重初始化为非常小的随机值。
W^[1]=np.random.randn((2,2))*0.01;
b^[1]=np.zero((2,1));
W^[2]=np.random.rand((1,2))*0.01
b[2]=0;<br>
这是因为当权重W过大时，会导致Z过大，使得激活函数接近饱和，从而减慢学习速度。
